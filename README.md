# Deep Research Agent

> Browser-powered deep research agent for AI coding assistants. No API keys required.

An [MCP](https://modelcontextprotocol.io/) server that gives any AI assistant the ability to search the web, read pages, take screenshots, and interact with sites â€” all through a real browser. Uses Bing for search and Playwright for browser automation, so there are **zero API keys** to configure.

## Features

- ğŸ” **Browser-based web search** â€” Bing via Playwright (stealth user-agent), no API keys needed
- ğŸ“„ **Full page content extraction** â€” JavaScript-rendered sites converted to clean Markdown via Readability + Turndown
- ğŸ“¸ **Screenshots** â€” capture full or viewport screenshots for visual analysis
- ğŸ–±ï¸ **Interactive navigation** â€” click elements, fill forms, follow links
- ğŸ”Œ **Replaceable search backend** â€” swap Bing for any search API by implementing the `SearchEngine` interface
- ğŸ¤– **Works with any MCP-compatible AI assistant** â€” Copilot CLI, Claude Code, Codex, VS Code, and more
- ğŸ“ **Structured logging** â€” all operations logged to stderr with timestamps, durations, and error context
- ğŸ§© **Research workflow prompt** â€” built-in `deep-research` prompt guides the host LLM through a complete multi-step research workflow

## Design

### How It Works

This project is an **MCP server** â€” it does **not** include its own LLM. Instead, it exposes browser-based research tools that a host AI assistant (Copilot CLI, Claude Code, Codex) orchestrates. The host LLM decides what to search, which pages to visit, and how to synthesize findings.

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Host LLM (Copilot CLI / Claude Code / Codex)              â”‚
â”‚  - Decides what to search, which pages to read             â”‚
â”‚  - Synthesizes findings into reports                       â”‚
â”‚  - Uses the deep-research prompt for guided workflows      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           â”‚ MCP Protocol (stdio)
           â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Deep Research Agent (this server)                          â”‚
â”‚                                                            â”‚
â”‚  Tools Layer          Services Layer                       â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚ web_search   â”‚â”€â”€â”€â–¶â”‚ SearchService                    â”‚  â”‚
â”‚  â”‚ visit_page   â”‚    â”‚  â””â”€ BingSearchEngine (default)   â”‚  â”‚
â”‚  â”‚ screenshot   â”‚â”€â”€â”€â–¶â”‚  â””â”€ DuckDuckGoEngine (alt)       â”‚  â”‚
â”‚  â”‚ click_elementâ”‚    â”‚  â””â”€ YourCustomEngine (plug in)   â”‚  â”‚
â”‚  â”‚ get_links    â”‚â”€â”€â”€â–¶â”‚ BrowserService (Playwright)      â”‚  â”‚
â”‚  â”‚ list_pages   â”‚    â”‚  â””â”€ Stealth user-agent           â”‚  â”‚
â”‚  â”‚ close_page   â”‚    â”‚  â””â”€ Page lifecycle management    â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚ ContentExtractor                  â”‚  â”‚
â”‚                      â”‚  â””â”€ Readability + Turndown        â”‚  â”‚
â”‚  Prompts Layer       â”‚  â””â”€ JSDOM (quiet virtual console) â”‚  â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚  â”‚deep-research â”‚    Logger (stderr, structured JSON)      â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           â”‚
           â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Chromium (headless, auto-installed by Playwright)          â”‚
â”‚  â””â”€ Bing search, page rendering, screenshots               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Key Design Decisions

| Decision | Rationale |
|----------|-----------|
| **MCP server, not standalone agent** | Universal compatibility â€” works with any MCP host (Copilot, Claude Code, Codex). The host LLM does the reasoning. |
| **Bing as default search engine** | DuckDuckGo serves CAPTCHAs to headless browsers. Google blocks by IP. Bing works reliably with a stealth user-agent. |
| **Stealth user-agent** | Search engines detect `HeadlessChrome`. We set `Chrome/131.0.0.0` + realistic viewport/locale to avoid bot detection. |
| **Replaceable search backend** | The `SearchEngine` interface lets you swap Bing for any API (Tavily, Brave, Exa) with a single class change. |
| **No LLM API dependency** | The host provides the LLM. Zero API keys, zero cost, works offline (except for web access). |
| **Readability + Turndown** | Mozilla Readability strips boilerplate, Turndown converts to Markdown â€” clean content for LLM consumption. |
| **Structured logging to stderr** | Stdout is reserved for MCP protocol. All logs go to stderr with `[timestamp] [LEVEL] [component] message {metadata}`. |

## Quick Install

### GitHub Copilot CLI

Add to your project's `.github/copilot-mcp.json` (or user-level MCP config):

```json
{
  "mcpServers": {
    "deep-research": {
      "command": "npx",
      "args": ["-y", "deep-research-agent"]
    }
  }
}
```

### Claude Code

```bash
claude mcp add deep-research -- npx -y deep-research-agent
```

### VS Code (Copilot Chat)

Add to your VS Code `settings.json`:

```json
{
  "mcp": {
    "servers": {
      "deep-research": {
        "command": "npx",
        "args": ["-y", "deep-research-agent"]
      }
    }
  }
}
```

### OpenAI Codex / Other MCP Hosts

Use the generic MCP server configuration:

```json
{
  "mcpServers": {
    "deep-research": {
      "command": "npx",
      "args": ["-y", "deep-research-agent"]
    }
  }
}
```

### Environment Variables

| Variable | Default | Description |
|----------|---------|-------------|
| `LOG_LEVEL` | `info` | Logging verbosity: `debug`, `info`, `warn`, `error` |

## Available Tools

### `web_search`

Search the web using Bing. Returns titles, URLs, and snippets.

| Parameter      | Type   | Required | Default | Description              |
| -------------- | ------ | -------- | ------- | ------------------------ |
| `query`        | string | yes      | â€”       | Search query             |
| `num_results`  | number | no       | 10      | Number of results to return |

### `visit_page`

Visit a URL and extract the page content as Markdown. Renders JavaScript before extraction.

| Parameter        | Type    | Required | Default | Description                  |
| ---------------- | ------- | -------- | ------- | ---------------------------- |
| `url`            | string  | yes      | â€”       | URL to visit                 |
| `extract_links`  | boolean | no       | false   | Also return an array of links |

### `take_screenshot`

Capture a screenshot of a page as a base64-encoded PNG.

| Parameter    | Type    | Required | Default | Description                    |
| ------------ | ------- | -------- | ------- | ------------------------------ |
| `url`        | string  | yes      | â€”       | URL to screenshot              |
| `full_page`  | boolean | no       | false   | Capture the full scrollable page |

### `click_element`

Click an element on an already-open page using a CSS selector.

| Parameter   | Type   | Required | Description                    |
| ----------- | ------ | -------- | ------------------------------ |
| `page_id`   | string | yes      | ID of an open page             |
| `selector`  | string | yes      | CSS selector of the element    |

### `get_page_links`

Extract all links from an open page.

| Parameter | Type   | Required | Description        |
| --------- | ------ | -------- | ------------------ |
| `page_id` | string | yes      | ID of an open page |

### `list_open_pages`

List all currently open browser pages. Takes no parameters.

### `close_page`

Close a browser page.

| Parameter | Type   | Required | Description          |
| --------- | ------ | -------- | -------------------- |
| `page_id` | string | yes      | ID of the page to close |

## Available Prompts

### `deep-research`

A guided research workflow template that instructs the assistant to perform iterative, multi-source research with built-in citation tracking and verification.

| Parameter | Type   | Required | Default    | Description                                                  |
| --------- | ------ | -------- | ---------- | ------------------------------------------------------------ |
| `topic`   | string | yes      | â€”          | The research topic or question                               |
| `depth`   | string | no       | `standard` | Research depth: `quick` (3 sub-questions), `standard` (5), or `deep` (7) |

The prompt guides the assistant through a structured workflow:

1. **Decompose** the topic into sub-questions
2. **Search** for each sub-question
3. **Extract** content from the most relevant results
4. **Cross-reference** findings across sources
5. **Fill gaps** with follow-up searches
6. **Synthesize** findings into a coherent answer
7. **Report** with inline citations and a source list

## Project Structure

```
src/
â”œâ”€â”€ index.ts                        # Entry point â€” shebang, stdio transport, signal handlers
â”œâ”€â”€ server.ts                       # MCP server wiring â€” creates services, registers tools/prompts
â”œâ”€â”€ logger.ts                       # Structured logger (stderr, level-based, JSON metadata)
â”œâ”€â”€ tools/
â”‚   â”œâ”€â”€ web-search.ts               # web_search tool
â”‚   â”œâ”€â”€ visit-page.ts               # visit_page tool
â”‚   â”œâ”€â”€ take-screenshot.ts          # take_screenshot tool
â”‚   â””â”€â”€ page-actions.ts             # click_element, get_page_links, list_open_pages, close_page
â”œâ”€â”€ prompts/
â”‚   â””â”€â”€ research-workflow.ts        # deep-research prompt (7-step guided workflow)
â””â”€â”€ services/
    â”œâ”€â”€ browser.ts                  # Playwright browser lifecycle + stealth settings
    â”œâ”€â”€ search-engine.ts            # SearchEngine interface + SearchService wrapper
    â”œâ”€â”€ content-extractor.ts        # HTML â†’ Markdown (Readability + Turndown + quiet JSDOM)
    â””â”€â”€ search-backends/
        â”œâ”€â”€ bing.ts                 # Default â€” Bing search with URL redirect decoding
        â””â”€â”€ duckduckgo.ts           # Alternative â€” DuckDuckGo (may CAPTCHA in some environments)
```

## Custom Search Backend

The search backend is replaceable. Implement the `SearchEngine` interface to use any search API:

```typescript
import { SearchEngine, SearchResult } from './services/search-engine.js';

class MyCustomSearch implements SearchEngine {
  name = 'my-search';

  async search(query: string, numResults?: number): Promise<SearchResult[]> {
    // Call your preferred search API (Tavily, Brave, Exa, etc.)
    const response = await fetch(`https://api.example.com/search?q=${query}`);
    const data = await response.json();

    return data.results.map((r: any) => ({
      title: r.title,
      url: r.url,
      snippet: r.description,
    }));
  }
}
```

Then update `src/server.ts` to use your backend:

```typescript
const searchEngine = new MyCustomSearch();  // instead of BingSearchEngine
```

## Known Issues & Remaining Work

### Search Engine Reliability

| Engine | Status | Notes |
|--------|--------|-------|
| **Bing** (default) | âœ… Works | Reliable with stealth user-agent. URLs decoded from Bing redirects. |
| **DuckDuckGo** | âš ï¸ CAPTCHAs | Serves "select all ducks" CAPTCHA to headless browsers. Kept as alternative. |
| **Google** | âŒ Blocked | Blocks headless Chrome by IP (`/sorry/` redirect). Not implemented. |

Bing may eventually start blocking headless browsers too. The replaceable backend architecture makes it easy to switch to a paid search API when needed.

### Not Yet Implemented

- **npm publish** â€” The package is not yet published to npm. Currently install from source. Run `npm publish` to make `npx -y deep-research-agent` work globally.
- **Parallel sub-question execution** â€” The `deep-research` prompt instructs the host LLM to research sequentially. Parallel tool calls depend on the host's multi-tool-call support.
- **Session persistence** â€” Browser pages are lost when the server restarts. No cross-session memory.
- **Token/cost budgeting** â€” No mechanism to limit how many pages the host LLM visits or how much content it extracts.
- **Rate limiting** â€” No throttling between rapid Bing searches. Heavy use may trigger Bing bot detection.
- **Authentication support** â€” Cannot log into sites that require authentication.
- **PDF/document extraction** â€” Only HTML pages are supported. PDFs, Word docs, etc. are not extracted.
- **Vector store / long-term memory** â€” No semantic storage of past research findings.

### Operational Notes

- **First tool call is slow (~3-8s)** â€” Chromium browser launch happens lazily on first use. Subsequent calls reuse the browser instance.
- **`networkidle` wait strategy** â€” `visit_page` and `take_screenshot` wait for network idle, which can be slow on heavy sites. Consider adding a timeout parameter.
- **Stderr logging** â€” All logs go to stderr (stdout is MCP protocol). Set `LOG_LEVEL=debug` for full detail, `LOG_LEVEL=error` for quiet operation.
- **JSDOM CSS warnings suppressed** â€” Modern CSS (`:has()`, nesting) triggers harmless `Could not parse CSS stylesheet` errors in JSDOM. These are silenced via `VirtualConsole`.

## Development

```bash
git clone <repo-url>
cd agent_mobile
npm install
npm run build
npm start
```

| Script          | Description                              |
| --------------- | ---------------------------------------- |
| `npm run build` | Compile TypeScript to `dist/`            |
| `npm start`     | Start the MCP server (stdio transport)   |
| `npm run dev`   | Build in watch mode for development      |

## Requirements

- **Node.js** â‰¥ 18
- **Chromium** â€” auto-installed by Playwright during `npm install`

## License

MIT
